{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbce527",
   "metadata": {},
   "source": [
    "## Problem 2 [40 points] - Chase Rensberger\n",
    " For this problem, you will need to learn to use software libraries for the following non-linear classifier types:\n",
    "\n",
    "    • Boosted Decision Trees (i.e., boosting with decision trees as weak learner)\n",
    "    • Random Forests\n",
    "    • Support Vector Machines with Gaussian Kernel\n",
    "    \n",
    "All of these are available in scikit-learn, although you may also use other external libraries (e.g., XGBoost 1 for boosted decision trees and LibSVM for SVMs). You are welcome to implement learning algorithms for these classifiers yourself, but this is neither required nor recommended.\n",
    "\n",
    "Use the non-linear classifiers from above for classification of Adult dataset. You can download the data from [a9a](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html) in libSVM data repository. The a9a data set comes with two files: the training data file a9a with 32,561 samples each with 123 features, and a9a.t with 16,281 test samples. Note that a9a data is in LibSVM format. In this format, each line takes the form 〈label〉 〈feature-id〉:〈feature-value〉 〈feature- id〉:〈feature-value〉 ..... This format is especially suitable for sparse datasets. Note that scikit-learn includes utility functions (e.g., load svmlight file) for loading datasets in the LibSVM format.\n",
    "\n",
    "For each of learning algorithms, you will need to set various hyperparameters (e.g., the type of kernel and regularization parameter for SVM; tree method, max depth, number of weak classifiers, etc for XG- Boost; number of estimators and min impurity decrease for Random Forests). Often there are defaults that make a good starting point, but you may need to adjust at least some of them to get good performance. Use hold-out validation or K-fold cross-validation to do this (scikit-learn has nice features to accomplish this, e.g., you may use train test split to split data into train and test data and sklearn.model selection for K-fold cross validation). Do not make any hyperparameter choices (or any other similar choices) based on the test set! You should only compute the test error rates after you have settled on hyperparameter settings and trained your three final classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0655427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import metrics\n",
    "from Problem2RandomForests import determine_random_forest_hp\n",
    "from Problem2SVMwGK import determine_SVM_hp\n",
    "from Problem2BoostedDecisionTrees import determine_xgboost_hp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99fc027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_svmlight_file(\"a9a.txt\")\n",
    "data_test = load_svmlight_file(\"a9a.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aaabbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First seperate our input files, we will pass our training data into our various classification functions\n",
    "X_train = data_train[0]\n",
    "y_train = data_train[1]\n",
    "X_test = data_test[0]\n",
    "y_test = data_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d57ca04",
   "metadata": {},
   "source": [
    "## Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_n_estimators = 100\n",
    "default_max_depth = None\n",
    "default_lambda=1\n",
    "default_learning_rate=1.0\n",
    "default_missing=0\n",
    "default_objective='binary:logistic' #Not modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_out = determine_xgboost_hp(default_n_estimators, default_max_depth, default_lambda, default_learning_rate, default_missing, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score with default values\n",
    "bdt_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters based on randomized search with 5 fold cross validation\n",
    "bdt_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ce547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score with best hyperparameters\n",
    "btd_out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2867e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters looked at during randomized search (I only selected some of these to go into my table for question 4)\n",
    "btd_out[3]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv test scores (test made from training data as per cv) seperated by split for each hyper parameter looked at(I only selected some of these to go into my table for question 4)\n",
    "[btd_out[3]['split0_test_score'], btd_out[3]['split1_test_score'], btd_out[3]['split2_test_score'], btd_out[3]['split3_test_score'], btd_out[3]['split4_test_score']]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18244dbd",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020aa892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default values\n",
    "default_n_estimators = 100\n",
    "default_bootstrap = True\n",
    "default_max_depth = None\n",
    "default_min_impurity_decrease = 0.0\n",
    "default_min_samples_leaf = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67923674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Function returns a tuple with (test score with default values, best params dicitionary, test score with best params, cross validation results).\n",
    "# This function will take roughly 5 minutes to run.x\n",
    "\n",
    "rf_out = determine_random_forest_hp(X_train, y_train, X_test, y_test, default_n_estimators, default_boostrap, default_max_depth, default_min_impurity_decrease, default_min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ec8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score with default values\n",
    "rf_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad91d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters based on randomized search with 5 fold cross validation\n",
    "rf_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ca1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score with best hyperparameters\n",
    "rf_out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d860458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters looked at during randomized search (I only selected some of these to go into my table for question 4)\n",
    "rf_out[3]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43185420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv test scores (test made from training data as per cv) seperated by split for each hyper parameter looked at(I only selected some of these to go into my table for question 4)\n",
    "[rf_out[3]['split0_test_score'], rf_out[3]['split1_test_score'], rf_out[3]['split2_test_score'], rf_out[3]['split3_test_score'], rf_out[3]['split4_test_score']]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6683f5",
   "metadata": {},
   "source": [
    "## Support Vector Machines with Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b49d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_kernel = 'rbf'\n",
    "default_gamma = 'scale'\n",
    "default_c = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fded0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_out = determine_SVM_hp(X_train, y_train, X_test, y_test, kernel=default_kernel, gamma=default_gamma, c=default_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score with default values\n",
    "svm_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters based on randomized search with 5 fold cross validation\n",
    "svm_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b34225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score with best hyperparameters\n",
    "svm_out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters looked at during randomized search (I only selected some of these to go into my table for question 4)\n",
    "svm_out[3]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b48d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv test scores (test made from training data as per cv) seperated by split for each hyper parameter looked at(I only selected some of these to go into my table for question 4)\n",
    "[svm_out[3]['split0_test_score'], rf_out[3]['split1_test_score'], svm_out[3]['split2_test_score'], svm_out[3]['split3_test_score'], svm_out[3]['split4_test_score']]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066e522",
   "metadata": {},
   "source": [
    "## Questions\n",
    "### 1. A brief description of each algorithm and how it works.\n",
    "\n",
    "**Boosted Decision Trees (specifically XGBoost)**\n",
    "is a modified version of decision trees which can be used for both regression and classification. It makes use of gradient boosting which essentially means that in each stage, we introduce a weak leaner (a decision stump where the output is dependent on a single feature) to compensate for the poor performance of already obtained weak leaners. This means that we are essentially doing boosting which is a method for producing accurate classifiers by combining classifiers which are only slightly better than a random guess with an update step that aims to minimize a loss function (gradient descent).\n",
    "\n",
    "**Random Forests**\n",
    "are similar to bootstrap aggregating(bagging) with decision trees in that it makes use of sampling with replacement from our data set and then constructing a decision tree for each sample and averaging their output in some way(majority voting). Random forest offers an improvement over this though by decorrelating our decision trees. We do this because in pure bagging with decision trees where we consider the same features for each sample, it is possible that there will be some feature(s) that are very important and cause very similar trees to form, undermining our efforts of averaging high variance models. Random Forests deals with this problem by selecting a random subset of features that each tree considers, ensuring the trees have greater variance. This will in turn make the average of our trees less variable and more reliable.\n",
    "\n",
    "**Support Vector Machines with Gaussian Kernel**\n",
    "is similar to perceptron in that we are often looking to linearly seperate some data. The key differents to SVMs is that there is some margin (hard or soft) that we are trying to maximize. The key idea to the algorithm we are running here is the guassian kernel, which makes use of a trick in order to force our data to be linearly seperable and that involves mapping our data into a higher dimension space (often called the feature space) and then solving the problem in the new space without any loss of correctness.\n",
    "\n",
    "### 2. Description of your training methodology, with enough details so that another machine learning enthusiast can reproduce the your results. You need to submit all the codes (python and Jupyter notebooks) to reproduce your code. Please use prefix Problem2*.py where you need to replace * with the name of non-linear classifier for your coding files.\n",
    "\n",
    "The main idea of my training methodolgy is to give each algorithm a wide range of values for each hyper parameter and then using a randomized cross validated search (run multiple times) in order to get a get parameters that are (ideally) significantly better than the defaults.\n",
    "\n",
    "I define 3 seperate functions in 3 seperate files corresponding to each algorithm. They are all located in the immediate working directory of this jupyter notebook. This notebook and every python file are meant to be runable without any adjustments so reproducing my results should be simple. In order to populate my table below I don't use any deterministic random state to ensure the data is exactly reproducable though, but any reruns should be very similar.\n",
    "\n",
    "Each function first trains the classifier on our data with no cross validation and default parameters and then scores that model on the test data. This approach gives us a baseline we want to improve. Each function then defines a grid of possible values for each hyperparamater and since doing a cross validated grid search on each parameter combination would take many hours to complete, I opted for a random search through our grid with 5 fold cross validation. Not that this step does not use our test data in any way.\n",
    "\n",
    "Each function then takes the best paramaters from this process and trains the classifier with these new parameters on our data and scores it on our test data. We can then compare this to the default and see if there is any improvement to the hyper parameter modification.\n",
    "\n",
    "Each function returns a tuple with the (default parameter test score, the best hyper parameters it found, the test score of our classifier trained on those best parameters, and our cross validation results which I largely just used to fill out the table for question 4).\n",
    "\n",
    "### 3. The list of hyperparameters and brief description of each hyperparameter you tuned in training, their default values, and the final hyperparameter settings you use to get the best result.\n",
    "\n",
    "**Boosted Decision Trees (XGBoost)**\n",
    "- Het\n",
    "- doaiwhjod\n",
    "\n",
    "**Random Forests**\n",
    "\n",
    "**\n",
    "\n",
    "### 4. Training error rates, hold-out or cross-validation error rates, and test error rates for your final classifiers. You are also encouraged to report other settings you tried with the accuracy it achieved (please make a table with a column with each hyperparamter and accuracy of configuration of parameters).\n",
    "\n",
    "### cross-validation error rates:\n",
    "\n",
    "**Boosted Decision Tree (specifically XGBoost)**\n",
    "\n",
    "\n",
    "**Random Forest**\n",
    "\n",
    "Cross validation error rates with optimal parameters\n",
    "\n",
    "Test error rates for final classifier:\n",
    "\n",
    "|n_estimators|boostrap|max_depth|min_impurity_decrease|min_samples_leaf|Test Error|\n",
    "|:----: |:----: |:----: |:----: |:----:|:----:|\n",
    "|150|True|110|0|2|1|\n",
    "\n",
    "Other settings and their test error\n",
    "\n",
    "|n_estimators|boostrap|max_depth|min_impurity_decrease|min_samples_leaf|Test Error|\n",
    "|:----: |:----: |:----: |:----: |:----:|:----:|\n",
    "|150|True|110|0|2|1|\n",
    "\n",
    "**Support Vector Machines with Gaussian Kernel (using LibSVM)**\n",
    "\n",
    "\n",
    "\n",
    "### 5. Please do your best to obtain the best achievable accuracy for each classifier on given dataset. Note: The amount of effort you put on tuning the parameters will be determined based on the discrepancy between the accuracy you get and the best achievable accuracy on a9a data for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156612b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
