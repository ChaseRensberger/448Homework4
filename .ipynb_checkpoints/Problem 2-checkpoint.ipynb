{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbce527",
   "metadata": {},
   "source": [
    "## Problem 2 [40 points] - Chase Rensberger\n",
    " For this problem, you will need to learn to use software libraries for the following non-linear classifier types:\n",
    "\n",
    "    • Boosted Decision Trees (i.e., boosting with decision trees as weak learner)\n",
    "    • Random Forests\n",
    "    • Support Vector Machines with Gaussian Kernel\n",
    "    \n",
    "All of these are available in scikit-learn, although you may also use other external libraries (e.g., XGBoost 1 for boosted decision trees and LibSVM for SVMs). You are welcome to implement learning algorithms for these classifiers yourself, but this is neither required nor recommended.\n",
    "\n",
    "Use the non-linear classifiers from above for classification of Adult dataset. You can download the data from [a9a](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html) in libSVM data repository. The a9a data set comes with two files: the training data file a9a with 32,561 samples each with 123 features, and a9a.t with 16,281 test samples. Note that a9a data is in LibSVM format. In this format, each line takes the form 〈label〉 〈feature-id〉:〈feature-value〉 〈feature- id〉:〈feature-value〉 ..... This format is especially suitable for sparse datasets. Note that scikit-learn includes utility functions (e.g., load svmlight file) for loading datasets in the LibSVM format.\n",
    "\n",
    "For each of learning algorithms, you will need to set various hyperparameters (e.g., the type of kernel and regularization parameter for SVM; tree method, max depth, number of weak classifiers, etc for XG- Boost; number of estimators and min impurity decrease for Random Forests). Often there are defaults that make a good starting point, but you may need to adjust at least some of them to get good performance. Use hold-out validation or K-fold cross-validation to do this (scikit-learn has nice features to accomplish this, e.g., you may use train test split to split data into train and test data and sklearn.model selection for K-fold cross validation). Do not make any hyperparameter choices (or any other similar choices) based on the test set! You should only compute the test error rates after you have settled on hyperparameter settings and trained your three final classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0655427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import metrics\n",
    "from Problem2RandomForests import determine_random_forest_hp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99fc027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_svmlight_file(\"a9a.txt\")\n",
    "data_test = load_svmlight_file(\"a9a.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aaabbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First seperate our input files, we will pass our training data into our various classification functions\n",
    "X_train = data_train[0]\n",
    "y_train = data_train[1]\n",
    "X_test = data_test[0]\n",
    "y_test = data_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d57ca04",
   "metadata": {},
   "source": [
    "## Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18244dbd",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020aa892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default values\n",
    "default_n_estimators = 100\n",
    "default_bootstrap = True\n",
    "default_max_depth = None\n",
    "default_min_impurity_decrease = 0.0\n",
    "default_min_samples_leaf = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67923674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Function returns a tuple with (test score with default values, best params dicitionary, test score with best params, cross validation results).\n",
    "# This function will take roughly 5 minutes to run.x\n",
    "\n",
    "rf_out = determine_random_forest_hp(X_train, y_train, X_test, y_test, default_n_estimators, default_boostrap, default_max_depth, default_min_impurity_decrease, default_min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ec8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score with default values\n",
    "rf_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad91d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters based on randomized search with 5 fold cross validation\n",
    "rf_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ca1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score with best hyperparameters\n",
    "rf_out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d860458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters looked at during randomized search (I only selected some of these to go into my table for question 4)\n",
    "rf_out[3]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43185420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv training score for each hyper parameter looked at(I only selected some of these to go into my table for question 4)\n",
    "rf_out[3][]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066e522",
   "metadata": {},
   "source": [
    "## Questions\n",
    "### 1. A brief description of each algorithm and how it works.\n",
    "\n",
    "**Boosted Decision Trees**\n",
    "\n",
    "\n",
    "\n",
    "**Random Forests**\n",
    "are similar to bootstrap aggregating(bagging) with decision trees in that it makes use of sampling with replacement from our data set and then constructing a decision tree for each sample and averaging their output in some way(majority voting). Random forest offers an improvement over this though by decorrelating our decision trees. We do this because in pure bagging with decision trees where we consider the same features for each sample, it is possible that there will be some feature(s) that are very important and cause very similar trees to form, undermining our efforts of averaging high variance models. Random Forests deals with this problem by selecting a random subset of features that each tree considers, ensuring the trees have greater variance. This will in turn make the average of our trees less variable and more reliable.\n",
    "\n",
    "**Support Vector Machines with Gaussian Kernel**\n",
    "\n",
    "### 2. Description of your training methodology, with enough details so that another machine learning enthusiast can reproduce the your results. You need to submit all the codes (python and Jupyter notebooks) to reproduce your code. Please use prefix Problem2*.py where you need to replace * with the name of non-linear classifier for your coding files.\n",
    "\n",
    "### 3. The list of hyperparameters and brief description of each hyperparameter you tuned in training, their default values, and the final hyperparameter settings you use to get the best result.\n",
    "\n",
    "### 4. Training error rates, hold-out or cross-validation error rates, and test error rates for your final classifiers. You are also encouraged to report other settings you tried with the accuracy it achieved (please make a table with a column with each hyperparamter and accuracy of configuration of parameters).\n",
    "\n",
    "### cross-validation error rates:\n",
    "**Random Forest**\n",
    "\n",
    "Cross validation error rates with optimal parameters\n",
    "\n",
    "|n_estimators|boostrap|max_depth|min_impurity_decrease|min_samples_leaf|Mean Training Error|\n",
    "|:----: |:----: |:----: |:----: |:----:|:----:|\n",
    "|150|True|110|0|2|1|\n",
    "\n",
    "### 5. Please do your best to obtain the best achievable accuracy for each classifier on given dataset. Note: The amount of effort you put on tuning the parameters will be determined based on the discrepancy between the accuracy you get and the best achievable accuracy on a9a data for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156612b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
