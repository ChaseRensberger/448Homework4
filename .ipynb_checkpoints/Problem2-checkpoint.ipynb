{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbce527",
   "metadata": {},
   "source": [
    "## Problem 2 [40 points] - Chase Rensberger\n",
    " For this problem, you will need to learn to use software libraries for the following non-linear classifier types:\n",
    "\n",
    "    • Boosted Decision Trees (i.e., boosting with decision trees as weak learner)\n",
    "    • Random Forests\n",
    "    • Support Vector Machines with Gaussian Kernel\n",
    "    \n",
    "All of these are available in scikit-learn, although you may also use other external libraries (e.g., XGBoost 1 for boosted decision trees and LibSVM for SVMs). You are welcome to implement learning algorithms for these classifiers yourself, but this is neither required nor recommended.\n",
    "\n",
    "Use the non-linear classifiers from above for classification of Adult dataset. You can download the data from [a9a](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html) in libSVM data repository. The a9a data set comes with two files: the training data file a9a with 32,561 samples each with 123 features, and a9a.t with 16,281 test samples. Note that a9a data is in LibSVM format. In this format, each line takes the form 〈label〉 〈feature-id〉:〈feature-value〉 〈feature- id〉:〈feature-value〉 ..... This format is especially suitable for sparse datasets. Note that scikit-learn includes utility functions (e.g., load svmlight file) for loading datasets in the LibSVM format.\n",
    "\n",
    "For each of learning algorithms, you will need to set various hyperparameters (e.g., the type of kernel and regularization parameter for SVM; tree method, max depth, number of weak classifiers, etc for XG- Boost; number of estimators and min impurity decrease for Random Forests). Often there are defaults that make a good starting point, but you may need to adjust at least some of them to get good performance. Use hold-out validation or K-fold cross-validation to do this (scikit-learn has nice features to accomplish this, e.g., you may use train test split to split data into train and test data and sklearn.model selection for K-fold cross validation). Do not make any hyperparameter choices (or any other similar choices) based on the test set! You should only compute the test error rates after you have settled on hyperparameter settings and trained your three final classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0655427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import metrics\n",
    "from Problem2RandomForests import determine_random_forest_hp\n",
    "from Problem2SVMwGK import determine_SVM_hp\n",
    "from Problem2BoostedDecisionTrees import determine_xgboost_hp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99fc027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_svmlight_file(\"a9a.txt\")\n",
    "data_test = load_svmlight_file(\"a9a.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aaabbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First seperate our input files, we will pass our training data into our various classification functions\n",
    "X_train = data_train[0]\n",
    "y_train = data_train[1]\n",
    "X_test = data_test[0]\n",
    "y_test = data_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d57ca04",
   "metadata": {},
   "source": [
    "## Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847174ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_n_estimators = 100\n",
    "default_max_depth = None\n",
    "default_lambda=None\n",
    "default_learning_rate=None\n",
    "default_missing=np.nan\n",
    "default_objective='binary:logistic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9609d2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[21:51:36] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2.5, n_estimators=750, reg_lambda=2.5; total time=  22.2s\n",
      "[21:51:58] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=90, missing=8.5, n_estimators=150, reg_lambda=4.0; total time=  24.0s\n",
      "[21:52:23] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=90, missing=8.5, n_estimators=150, reg_lambda=4.0; total time=  15.0s\n",
      "[21:52:37] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=90, missing=8.5, n_estimators=150, reg_lambda=4.0; total time=  12.2s\n",
      "[21:52:50] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=150, missing=5.5, n_estimators=100, reg_lambda=0.5; total time= 2.5min\n",
      "[21:55:18] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2, n_estimators=50, reg_lambda=2.0; total time=   7.0s\n",
      "[21:55:25] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2, n_estimators=50, reg_lambda=2.0; total time=   8.2s\n",
      "[21:55:33] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2, n_estimators=50, reg_lambda=2.0; total time=   7.2s\n",
      "[21:55:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2, n_estimators=50, reg_lambda=2.0; total time=   7.9s\n",
      "[21:55:49] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2, n_estimators=50, reg_lambda=2.0; total time=   7.6s\n",
      "[21:55:56] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=4.0, max_depth=80, missing=5.5, n_estimators=150, reg_lambda=4.0; total time=   9.1s\n",
      "[21:56:05] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=4.0, max_depth=80, missing=5.5, n_estimators=150, reg_lambda=4.0; total time=   9.2s\n",
      "[21:56:14] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=4.0, max_depth=80, missing=5.5, n_estimators=150, reg_lambda=4.0; total time=   9.4s\n",
      "[21:56:24] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=4.0, max_depth=80, missing=5.5, n_estimators=150, reg_lambda=4.0; total time=  10.1s\n",
      "[21:56:34] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=4.0, max_depth=80, missing=5.5, n_estimators=150, reg_lambda=4.0; total time=   9.8s\n",
      "[21:56:44] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.0, max_depth=140, missing=9, n_estimators=450, reg_lambda=1.5; total time= 6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:703: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:51:36] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2.5, n_estimators=750, reg_lambda=2.5; total time=  21.8s\n",
      "[21:51:58] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2.5, n_estimators=750, reg_lambda=2.5; total time=  28.4s\n",
      "[21:52:26] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=90, missing=8.5, n_estimators=150, reg_lambda=4.0; total time=  14.2s\n",
      "[21:52:41] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=150, missing=5.5, n_estimators=100, reg_lambda=0.5; total time= 2.4min\n",
      "[21:55:08] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=150, missing=5.5, n_estimators=100, reg_lambda=0.5; total time= 2.6min\n",
      "[21:57:43] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.0, max_depth=140, missing=9, n_estimators=450, reg_lambda=1.5; total time= 6.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:51:36] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2.5, n_estimators=750, reg_lambda=2.5; total time=  21.0s\n",
      "[21:51:57] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=140, missing=2.5, n_estimators=750, reg_lambda=2.5; total time=  29.6s\n",
      "[21:52:26] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=3.5, max_depth=90, missing=8.5, n_estimators=150, reg_lambda=4.0; total time=  17.5s\n",
      "[21:52:44] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=150, missing=5.5, n_estimators=100, reg_lambda=0.5; total time= 2.5min\n",
      "[21:55:16] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=150, missing=5.5, n_estimators=100, reg_lambda=0.5; total time= 2.6min\n",
      "[21:57:50] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.0, max_depth=140, missing=9, n_estimators=450, reg_lambda=1.5; total time= 6.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:08:23] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:08:29] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# Will throw a warning that for the life of me I can't get rid of, this can be ignored though\n",
    "bdt_out = determine_xgboost_hp(X_train, y_train, X_test, y_test, default_n_estimators, default_max_depth, default_lambda, default_learning_rate, default_missing, default_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8ecf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8331183588231681"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score with default values\n",
    "bdt_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4add5b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_lambda': 3.5,\n",
       " 'n_estimators': 600,\n",
       " 'missing': 8.5,\n",
       " 'max_depth': None,\n",
       " 'learning_rate': 0.5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameters based on randomized search with 5 fold cross validation\n",
    "bdt_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48415ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8316442478963209"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score with best hyperparameters (Note that this is the score found from one iteration of randomized search and the score below may be even below the default values, I used a bunch of iterations of this randomized search along with some of my own playing around with values to get a decent result)\n",
    "bdt_out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2ec5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reg_lambda': 4.0,\n",
       "  'n_estimators': 700,\n",
       "  'missing': 0.5,\n",
       "  'max_depth': 50,\n",
       "  'learning_rate': 1.5},\n",
       " {'reg_lambda': 2.5,\n",
       "  'n_estimators': 750,\n",
       "  'missing': 2.5,\n",
       "  'max_depth': 140,\n",
       "  'learning_rate': 3.5},\n",
       " {'reg_lambda': 4.0,\n",
       "  'n_estimators': 150,\n",
       "  'missing': 8.5,\n",
       "  'max_depth': 90,\n",
       "  'learning_rate': 3.5},\n",
       " {'reg_lambda': 0.5,\n",
       "  'n_estimators': 100,\n",
       "  'missing': 5.5,\n",
       "  'max_depth': 150,\n",
       "  'learning_rate': 0.5},\n",
       " {'reg_lambda': 2.0,\n",
       "  'n_estimators': 50,\n",
       "  'missing': 2,\n",
       "  'max_depth': 140,\n",
       "  'learning_rate': 3.5},\n",
       " {'reg_lambda': 4.0,\n",
       "  'n_estimators': 150,\n",
       "  'missing': 5.5,\n",
       "  'max_depth': 80,\n",
       "  'learning_rate': 4.0},\n",
       " {'reg_lambda': 1.5,\n",
       "  'n_estimators': 450,\n",
       "  'missing': 9,\n",
       "  'max_depth': 140,\n",
       "  'learning_rate': 2.0},\n",
       " {'reg_lambda': 2.0,\n",
       "  'n_estimators': 200,\n",
       "  'missing': 8.5,\n",
       "  'max_depth': 80,\n",
       "  'learning_rate': 2.5},\n",
       " {'reg_lambda': 3.0,\n",
       "  'n_estimators': 200,\n",
       "  'missing': 9.5,\n",
       "  'max_depth': 130,\n",
       "  'learning_rate': 0.5},\n",
       " {'reg_lambda': 3.5,\n",
       "  'n_estimators': 600,\n",
       "  'missing': 8.5,\n",
       "  'max_depth': None,\n",
       "  'learning_rate': 0.5}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper parameters looked at during randomized search\n",
    "bdt_out[3]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f1147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.81498541, 0.75955781, 0.71733456, 0.81621373, 0.74819592,\n",
       "        0.74297559, 0.79441118, 0.7933364 , 0.81867035, 0.82634731]),\n",
       " array([0.82171376, 0.75568182, 0.75921376, 0.81772113, 0.76351351,\n",
       "        0.74293612, 0.80374693, 0.7870086 , 0.81971744, 0.82985258]),\n",
       " array([0.8252457 , 0.75859951, 0.6007371 , 0.82386364, 0.7906941 ,\n",
       "        0.74385749, 0.80712531, 0.79130835, 0.82754914, 0.83338452]),\n",
       " array([0.82309582, 0.77933047, 0.67997543, 0.82340295, 0.77195946,\n",
       "        0.75644963, 0.80482187, 0.77948403, 0.82954545, 0.83737715]),\n",
       " array([0.82447789, 0.77149877, 0.74662162, 0.83015971, 0.70300983,\n",
       "        0.75890663, 0.814957  , 0.78178747, 0.82601351, 0.83614865])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv test scores (test meaing not the actual test data and the data suplied by cv) seperated by split for each hyper parameter looked at.\n",
    "[bdt_out[3]['split0_test_score'], bdt_out[3]['split1_test_score'], bdt_out[3]['split2_test_score'], bdt_out[3]['split3_test_score'], bdt_out[3]['split4_test_score']]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18244dbd",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020aa892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default values\n",
    "default_n_estimators = 100\n",
    "default_bootstrap = True\n",
    "default_max_depth = None\n",
    "default_min_impurity_decrease = 0.0\n",
    "default_min_samples_leaf = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67923674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "# Function returns a tuple with (test score with default values, best params dicitionary, test score with best params, cross validation results).\n",
    "# This function will take roughly 2.5 minutes to run.x\n",
    "\n",
    "rf_out = determine_random_forest_hp(X_train, y_train, X_test, y_test, default_n_estimators, default_bootstrap, default_max_depth, default_min_impurity_decrease, default_min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "974ec8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8331183588231681"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score with default values\n",
    "rf_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ad91d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 950,\n",
       " 'min_samples_leaf': 5,\n",
       " 'min_impurity_decrease': 0,\n",
       " 'max_depth': 90,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameters based on randomized search with 5 fold cross validation\n",
    "rf_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a81ca1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8492107364412506"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score with best hyperparameters (Note that this is the score found from one iteration of randomized search and the score below may be even below the default values, I used a bunch of iterations of this randomized search along with some of my own playing around with values to get a decent result)\n",
    "rf_out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d860458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_estimators': 350,\n",
       "  'min_samples_leaf': 6,\n",
       "  'min_impurity_decrease': 8,\n",
       "  'max_depth': 50,\n",
       "  'bootstrap': True},\n",
       " {'n_estimators': 450,\n",
       "  'min_samples_leaf': 4,\n",
       "  'min_impurity_decrease': 3,\n",
       "  'max_depth': 30,\n",
       "  'bootstrap': True},\n",
       " {'n_estimators': 150,\n",
       "  'min_samples_leaf': 7,\n",
       "  'min_impurity_decrease': 4,\n",
       "  'max_depth': 110,\n",
       "  'bootstrap': False},\n",
       " {'n_estimators': 550,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_impurity_decrease': 4,\n",
       "  'max_depth': 100,\n",
       "  'bootstrap': True},\n",
       " {'n_estimators': 650,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_impurity_decrease': 10,\n",
       "  'max_depth': 30,\n",
       "  'bootstrap': False},\n",
       " {'n_estimators': 300,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_impurity_decrease': 9,\n",
       "  'max_depth': 90,\n",
       "  'bootstrap': False},\n",
       " {'n_estimators': 950,\n",
       "  'min_samples_leaf': 5,\n",
       "  'min_impurity_decrease': 0,\n",
       "  'max_depth': 90,\n",
       "  'bootstrap': True},\n",
       " {'n_estimators': 300,\n",
       "  'min_samples_leaf': 2,\n",
       "  'min_impurity_decrease': 9,\n",
       "  'max_depth': 130,\n",
       "  'bootstrap': False},\n",
       " {'n_estimators': 300,\n",
       "  'min_samples_leaf': 10,\n",
       "  'min_impurity_decrease': 7,\n",
       "  'max_depth': 15,\n",
       "  'bootstrap': True},\n",
       " {'n_estimators': 200,\n",
       "  'min_samples_leaf': 6,\n",
       "  'min_impurity_decrease': 2,\n",
       "  'max_depth': 10,\n",
       "  'bootstrap': True}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper parameters looked at during randomized search\n",
    "rf_out[3]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43185420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.75909719, 0.75909719, 0.75909719, 0.75909719, 0.75909719,\n",
       "        0.75909719, 0.8404729 , 0.75909719, 0.75909719, 0.75909719]),\n",
       " array([0.75921376, 0.75921376, 0.75921376, 0.75921376, 0.75921376,\n",
       "        0.75921376, 0.84152334, 0.75921376, 0.75921376, 0.75921376]),\n",
       " array([0.75921376, 0.75921376, 0.75921376, 0.75921376, 0.75921376,\n",
       "        0.75921376, 0.84689803, 0.75921376, 0.75921376, 0.75921376]),\n",
       " array([0.75921376, 0.75921376, 0.75921376, 0.75921376, 0.75921376,\n",
       "        0.75921376, 0.8495086 , 0.75921376, 0.75921376, 0.75921376]),\n",
       " array([0.75921376, 0.75921376, 0.75921376, 0.75921376, 0.75921376,\n",
       "        0.75921376, 0.84766585, 0.75921376, 0.75921376, 0.75921376])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv test scores (test meaing not the actual test data and the data suplied by cv) seperated by split for each hyper parameter looked at.\n",
    "[rf_out[3]['split0_test_score'], rf_out[3]['split1_test_score'], rf_out[3]['split2_test_score'], rf_out[3]['split3_test_score'], rf_out[3]['split4_test_score']]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d3ef03",
   "metadata": {},
   "source": [
    "## Support Vector Machines with Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8138c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_kernel = 'rbf'\n",
    "default_gamma = 'scale'\n",
    "default_c = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "192ddbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[22:04:19] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=None, missing=8.5, n_estimators=600, reg_lambda=3.5; total time= 1.0min\n",
      "[22:05:21] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=None, missing=8.5, n_estimators=600, reg_lambda=3.5; total time= 1.1min\n",
      "[22:06:26] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=None, missing=8.5, n_estimators=600, reg_lambda=3.5; total time= 1.1min\n",
      "[22:07:34] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=None, missing=8.5, n_estimators=600, reg_lambda=3.5; total time=  48.3s\n",
      "[CV] END bootstrap=True, max_depth=50, min_impurity_decrease=8, min_samples_leaf=6, n_estimators=350; total time=   1.4s\n",
      "[CV] END bootstrap=False, max_depth=110, min_impurity_decrease=4, min_samples_leaf=7, n_estimators=150; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=100, min_impurity_decrease=4, min_samples_leaf=1, n_estimators=550; total time=   2.1s\n",
      "[CV] END bootstrap=False, max_depth=90, min_impurity_decrease=9, min_samples_leaf=1, n_estimators=300; total time=   1.0s\n",
      "[CV] END bootstrap=True, max_depth=90, min_impurity_decrease=0, min_samples_leaf=5, n_estimators=950; total time=  57.5s\n",
      "[CV] END .....................C=2.0, gamma=scale, kernel=rbf; total time=  29.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:703: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "svm_out = determine_SVM_hp(X_train, y_train, X_test, y_test, default_kernel, default_gamma, default_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91f9d54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8505620047908605"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score with default values\n",
    "svm_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6e57a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'rbf', 'gamma': 'scale', 'C': 2.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameters based on randomized search with 5 fold cross validation\n",
    "svm_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d301592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8505620047908605"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test score with best hyperparameters (Note that this is the score found from one iteration of randomized search and the score below may be even below the default values, I used a bunch of iterations of this randomized search along with some of my own playing around with values to get a decent result)\n",
    "svm_out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47cf7f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'kernel': 'rbf', 'gamma': 'scale', 'C': 2.0},\n",
       " {'kernel': 'linear', 'gamma': 'auto', 'C': 1.5},\n",
       " {'kernel': 'sigmoid', 'gamma': 'scale', 'C': 3.0},\n",
       " {'kernel': 'linear', 'gamma': 'auto', 'C': 2.5},\n",
       " {'kernel': 'linear', 'gamma': 'auto', 'C': 0.5},\n",
       " {'kernel': 'poly', 'gamma': 'scale', 'C': 2.0},\n",
       " {'kernel': 'sigmoid', 'gamma': 'auto', 'C': 6.5},\n",
       " {'kernel': 'poly', 'gamma': 'auto', 'C': 2.5},\n",
       " {'kernel': 'sigmoid', 'gamma': 'scale', 'C': 0.5},\n",
       " {'kernel': 'poly', 'gamma': 'scale', 'C': 5.5}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper parameters looked at during randomized search\n",
    "svm_out[3]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaa4bc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.84431138, 0.84446492, 0.77506525, 0.84461846, 0.84431138,\n",
       "        0.84216183, 0.84308306, 0.77644711, 0.78688776, 0.8369415 ]),\n",
       " array([0.75921376, 0.75921376, 0.75921376, 0.75921376, 0.75921376,\n",
       "        0.75921376, 0.84152334, 0.75921376, 0.75921376, 0.75921376]),\n",
       " array([0.85027641, 0.84628378, 0.78148034, 0.84659091, 0.84613022,\n",
       "        0.84843366, 0.84551597, 0.77764128, 0.76842752, 0.84413391]),\n",
       " array([0.85273342, 0.84904791, 0.78286241, 0.84874079, 0.84920147,\n",
       "        0.85288698, 0.84720516, 0.77472359, 0.78194103, 0.84643735]),\n",
       " array([0.84751229, 0.84904791, 0.77702703, 0.84874079, 0.84981572,\n",
       "        0.85089066, 0.84920147, 0.77825553, 0.79054054, 0.84367322])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:04:12] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=130, missing=9.5, n_estimators=200, reg_lambda=3.0; total time= 4.2min\n",
      "[CV] END bootstrap=True, max_depth=50, min_impurity_decrease=8, min_samples_leaf=6, n_estimators=350; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=110, min_impurity_decrease=4, min_samples_leaf=7, n_estimators=150; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=100, min_impurity_decrease=4, min_samples_leaf=1, n_estimators=550; total time=   2.0s\n",
      "[CV] END bootstrap=False, max_depth=30, min_impurity_decrease=10, min_samples_leaf=1, n_estimators=650; total time=   1.9s\n",
      "[CV] END bootstrap=False, max_depth=130, min_impurity_decrease=9, min_samples_leaf=2, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=130, min_impurity_decrease=9, min_samples_leaf=2, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=15, min_impurity_decrease=7, min_samples_leaf=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=10, min_impurity_decrease=2, min_samples_leaf=6, n_estimators=200; total time=   0.7s\n",
      "[CV] END bootstrap=True, max_depth=10, min_impurity_decrease=2, min_samples_leaf=6, n_estimators=200; total time=   0.7s\n",
      "[CV] END .....................C=2.0, gamma=scale, kernel=rbf; total time=  30.2s\n",
      "[CV] END .................C=3.0, gamma=scale, kernel=sigmoid; total time=  19.2s\n",
      "[CV] END ...................C=2.5, gamma=auto, kernel=linear; total time=  48.6s\n",
      "[CV] END ....................C=2.0, gamma=scale, kernel=poly; total time=  36.4s\n",
      "[CV] END ..................C=6.5, gamma=auto, kernel=sigmoid; total time=  21.3s\n",
      "[CV] END .................C=0.5, gamma=scale, kernel=sigmoid; total time=  26.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:51:36] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=1.5, max_depth=50, missing=0.5, n_estimators=700, reg_lambda=4.0; total time=10.3min\n",
      "[22:01:52] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.0, max_depth=140, missing=9, n_estimators=450, reg_lambda=1.5; total time= 6.5min\n",
      "[CV] END bootstrap=True, max_depth=30, min_impurity_decrease=3, min_samples_leaf=4, n_estimators=450; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=100, min_impurity_decrease=4, min_samples_leaf=1, n_estimators=550; total time=   2.0s\n",
      "[CV] END bootstrap=False, max_depth=30, min_impurity_decrease=10, min_samples_leaf=1, n_estimators=650; total time=   1.9s\n",
      "[CV] END bootstrap=False, max_depth=130, min_impurity_decrease=9, min_samples_leaf=2, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=130, min_impurity_decrease=9, min_samples_leaf=2, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=15, min_impurity_decrease=7, min_samples_leaf=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=15, min_impurity_decrease=7, min_samples_leaf=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=10, min_impurity_decrease=2, min_samples_leaf=6, n_estimators=200; total time=   0.8s\n",
      "[CV] END .....................C=2.0, gamma=scale, kernel=rbf; total time=  29.8s\n",
      "[CV] END .................C=3.0, gamma=scale, kernel=sigmoid; total time=  20.3s\n",
      "[CV] END ...................C=2.5, gamma=auto, kernel=linear; total time=  49.6s\n",
      "[CV] END ....................C=2.0, gamma=scale, kernel=poly; total time=  35.9s\n",
      "[CV] END .....................C=2.5, gamma=auto, kernel=poly; total time=  26.8s\n",
      "[CV] END .................C=0.5, gamma=scale, kernel=sigmoid; total time=  19.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:03:10] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=130, missing=9.5, n_estimators=200, reg_lambda=3.0; total time= 4.5min\n",
      "[CV] END bootstrap=True, max_depth=50, min_impurity_decrease=8, min_samples_leaf=6, n_estimators=350; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=30, min_impurity_decrease=3, min_samples_leaf=4, n_estimators=450; total time=   1.6s\n",
      "[CV] END bootstrap=False, max_depth=30, min_impurity_decrease=10, min_samples_leaf=1, n_estimators=650; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=90, min_impurity_decrease=0, min_samples_leaf=5, n_estimators=950; total time=  57.5s\n",
      "[CV] END ...................C=1.5, gamma=auto, kernel=linear; total time=  37.6s\n",
      "[CV] END ...................C=2.5, gamma=auto, kernel=linear; total time=  47.8s\n",
      "[CV] END ...................C=0.5, gamma=auto, kernel=linear; total time=  25.3s\n",
      "[CV] END ..................C=6.5, gamma=auto, kernel=sigmoid; total time=  22.6s\n",
      "[CV] END .....................C=2.5, gamma=auto, kernel=poly; total time=  27.3s\n",
      "[CV] END .................C=0.5, gamma=scale, kernel=sigmoid; total time=  40.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:51:36] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=1.5, max_depth=50, missing=0.5, n_estimators=700, reg_lambda=4.0; total time=10.4min\n",
      "[22:02:01] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.5, max_depth=80, missing=8.5, n_estimators=200, reg_lambda=2.0; total time=  40.5s\n",
      "[22:02:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.5, max_depth=80, missing=8.5, n_estimators=200, reg_lambda=2.0; total time=  44.6s\n",
      "[22:03:26] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=130, missing=9.5, n_estimators=200, reg_lambda=3.0; total time= 4.5min\n",
      "[CV] END bootstrap=True, max_depth=30, min_impurity_decrease=3, min_samples_leaf=4, n_estimators=450; total time=   1.7s\n",
      "[CV] END bootstrap=False, max_depth=110, min_impurity_decrease=4, min_samples_leaf=7, n_estimators=150; total time=   0.5s\n",
      "[CV] END bootstrap=False, max_depth=30, min_impurity_decrease=10, min_samples_leaf=1, n_estimators=650; total time=   1.9s\n",
      "[CV] END bootstrap=False, max_depth=90, min_impurity_decrease=9, min_samples_leaf=1, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=90, min_impurity_decrease=0, min_samples_leaf=5, n_estimators=950; total time=  57.7s\n",
      "[CV] END .....................C=2.0, gamma=scale, kernel=rbf; total time=  29.1s\n",
      "[CV] END ...................C=1.5, gamma=auto, kernel=linear; total time=  35.7s\n",
      "[CV] END ...................C=0.5, gamma=auto, kernel=linear; total time=  27.8s\n",
      "[CV] END ....................C=2.0, gamma=scale, kernel=poly; total time=  40.3s\n",
      "[CV] END .....................C=2.5, gamma=auto, kernel=poly; total time=  26.6s\n",
      "[CV] END ....................C=5.5, gamma=scale, kernel=poly; total time=  53.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:51:36] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=1.5, max_depth=50, missing=0.5, n_estimators=700, reg_lambda=4.0; total time=10.4min\n",
      "[22:01:58] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.5, max_depth=80, missing=8.5, n_estimators=200, reg_lambda=2.0; total time=  47.6s\n",
      "[22:02:45] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=130, missing=9.5, n_estimators=200, reg_lambda=3.0; total time= 4.4min\n",
      "[22:07:09] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=None, missing=8.5, n_estimators=600, reg_lambda=3.5; total time= 1.0min\n",
      "[CV] END bootstrap=True, max_depth=50, min_impurity_decrease=8, min_samples_leaf=6, n_estimators=350; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=30, min_impurity_decrease=3, min_samples_leaf=4, n_estimators=450; total time=   1.7s\n",
      "[CV] END bootstrap=False, max_depth=30, min_impurity_decrease=10, min_samples_leaf=1, n_estimators=650; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=90, min_impurity_decrease=0, min_samples_leaf=5, n_estimators=950; total time=  57.5s\n",
      "[CV] END ...................C=1.5, gamma=auto, kernel=linear; total time=  36.9s\n",
      "[CV] END .................C=3.0, gamma=scale, kernel=sigmoid; total time=  18.2s\n",
      "[CV] END ...................C=2.5, gamma=auto, kernel=linear; total time=  49.6s\n",
      "[CV] END ....................C=2.0, gamma=scale, kernel=poly; total time=  34.7s\n",
      "[CV] END .....................C=2.5, gamma=auto, kernel=poly; total time=  27.1s\n",
      "[CV] END ....................C=5.5, gamma=scale, kernel=poly; total time=  54.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:51:36] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=1.5, max_depth=50, missing=0.5, n_estimators=700, reg_lambda=4.0; total time=10.2min\n",
      "[22:01:48] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.0, max_depth=140, missing=9, n_estimators=450, reg_lambda=1.5; total time= 6.6min\n",
      "[CV] END bootstrap=True, max_depth=30, min_impurity_decrease=3, min_samples_leaf=4, n_estimators=450; total time=   1.7s\n",
      "[CV] END bootstrap=False, max_depth=110, min_impurity_decrease=4, min_samples_leaf=7, n_estimators=150; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=100, min_impurity_decrease=4, min_samples_leaf=1, n_estimators=550; total time=   2.1s\n",
      "[CV] END bootstrap=False, max_depth=90, min_impurity_decrease=9, min_samples_leaf=1, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=90, min_impurity_decrease=0, min_samples_leaf=5, n_estimators=950; total time=  56.9s\n",
      "[CV] END ...................C=1.5, gamma=auto, kernel=linear; total time=  37.2s\n",
      "[CV] END .................C=3.0, gamma=scale, kernel=sigmoid; total time=  20.4s\n",
      "[CV] END ...................C=2.5, gamma=auto, kernel=linear; total time=  48.4s\n",
      "[CV] END ....................C=2.0, gamma=scale, kernel=poly; total time=  35.8s\n",
      "[CV] END .....................C=2.5, gamma=auto, kernel=poly; total time=  26.4s\n",
      "[CV] END ....................C=5.5, gamma=scale, kernel=poly; total time=  53.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................C=3.0, gamma=scale, kernel=sigmoid; total time=  42.8s\n",
      "[CV] END ...................C=0.5, gamma=auto, kernel=linear; total time=  28.7s\n",
      "[CV] END ..................C=6.5, gamma=auto, kernel=sigmoid; total time=  22.1s\n",
      "[CV] END ..................C=6.5, gamma=auto, kernel=sigmoid; total time=  22.3s\n",
      "[CV] END .................C=0.5, gamma=scale, kernel=sigmoid; total time=  23.2s\n",
      "[CV] END ....................C=5.5, gamma=scale, kernel=poly; total time=  44.0s\n",
      "[21:51:36] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=1.5, max_depth=50, missing=0.5, n_estimators=700, reg_lambda=4.0; total time=10.3min\n",
      "[22:01:54] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.5, max_depth=80, missing=8.5, n_estimators=200, reg_lambda=2.0; total time=  50.3s\n",
      "[22:02:44] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=2.5, max_depth=80, missing=8.5, n_estimators=200, reg_lambda=2.0; total time=  49.3s\n",
      "[22:03:33] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.5, max_depth=130, missing=9.5, n_estimators=200, reg_lambda=3.0; total time= 4.5min\n",
      "[CV] END bootstrap=True, max_depth=50, min_impurity_decrease=8, min_samples_leaf=6, n_estimators=350; total time=   1.3s\n",
      "[CV] END bootstrap=False, max_depth=110, min_impurity_decrease=4, min_samples_leaf=7, n_estimators=150; total time=   0.5s\n",
      "[CV] END bootstrap=True, max_depth=100, min_impurity_decrease=4, min_samples_leaf=1, n_estimators=550; total time=   2.1s\n",
      "[CV] END bootstrap=False, max_depth=90, min_impurity_decrease=9, min_samples_leaf=1, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=90, min_impurity_decrease=9, min_samples_leaf=1, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=False, max_depth=130, min_impurity_decrease=9, min_samples_leaf=2, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=15, min_impurity_decrease=7, min_samples_leaf=10, n_estimators=300; total time=   1.1s\n",
      "[CV] END bootstrap=True, max_depth=15, min_impurity_decrease=7, min_samples_leaf=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=10, min_impurity_decrease=2, min_samples_leaf=6, n_estimators=200; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=10, min_impurity_decrease=2, min_samples_leaf=6, n_estimators=200; total time=   0.7s\n",
      "[CV] END .....................C=2.0, gamma=scale, kernel=rbf; total time=  27.7s\n",
      "[CV] END ...................C=1.5, gamma=auto, kernel=linear; total time=  34.9s\n",
      "[CV] END ...................C=0.5, gamma=auto, kernel=linear; total time=  24.3s\n",
      "[CV] END ...................C=0.5, gamma=auto, kernel=linear; total time=  26.8s\n",
      "[CV] END ..................C=6.5, gamma=auto, kernel=sigmoid; total time=  19.9s\n",
      "[CV] END .................C=0.5, gamma=scale, kernel=sigmoid; total time=  41.4s\n",
      "[CV] END ....................C=5.5, gamma=scale, kernel=poly; total time=  50.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/chr/opt/anaconda3/envs/CMPSC448/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# cv test scores (test meaing not the actual test data and the data suplied by cv) seperated by split for each hyper parameter looked at.\n",
    "[svm_out[3]['split0_test_score'], rf_out[3]['split1_test_score'], svm_out[3]['split2_test_score'], svm_out[3]['split3_test_score'], svm_out[3]['split4_test_score']]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066e522",
   "metadata": {},
   "source": [
    "## Questions\n",
    "### 1. A brief description of each algorithm and how it works.\n",
    "\n",
    "**Boosted Decision Trees (specifically XGBoost)**\n",
    "is a modified version of decision trees which can be used for both regression and classification. It makes use of gradient boosting which essentially means that in each stage, we introduce a weak leaner (a decision stump where the output is dependent on a single feature) to compensate for the poor performance of already obtained weak leaners. This means that we are essentially doing boosting which is a method for producing accurate classifiers by combining classifiers which are only slightly better than a random guess with an update step that aims to minimize a loss function (gradient descent).\n",
    "\n",
    "**Random Forests**\n",
    "are similar to bootstrap aggregating(bagging) with decision trees in that it makes use of sampling with replacement from our data set and then constructing a decision tree for each sample and averaging their output in some way(majority voting). Random forest offers an improvement over this though by decorrelating our decision trees. We do this because in pure bagging with decision trees where we consider the same features for each sample, it is possible that there will be some feature(s) that are very important and cause very similar trees to form, undermining our efforts of averaging high variance models. Random Forests deals with this problem by selecting a random subset of features that each tree considers, ensuring the trees have greater variance. This will in turn make the average of our trees less variable and more reliable.\n",
    "\n",
    "**Support Vector Machines with Gaussian Kernel**\n",
    "is similar to perceptron in that we are often looking to linearly seperate some data. The key differents to SVMs is that there is some margin (hard or soft) that we are trying to maximize. The key idea to the algorithm we are running here is the guassian kernel, which makes use of a trick in order to force our data to be linearly seperable and that involves mapping our data into a higher dimension space (often called the feature space) and then solving the problem in the new space without any loss of correctness.\n",
    "\n",
    "### 2. Description of your training methodology, with enough details so that another machine learning enthusiast can reproduce the your results. You need to submit all the codes (python and Jupyter notebooks) to reproduce your code. Please use prefix Problem2*.py where you need to replace * with the name of non-linear classifier for your coding files.\n",
    "\n",
    "The main idea of my training methodolgy is to give each algorithm a wide range of values for each hyper parameter and then using a randomized cross validated search (run multiple times) in order to get a get parameters that are (ideally) significantly better than the defaults.\n",
    "\n",
    "I define 3 seperate functions in 3 seperate files corresponding to each algorithm. They are all located in the immediate working directory of this jupyter notebook. This notebook and every python file are meant to be runable without any adjustments so reproducing my results should be simple. I don't use any deterministic random state to ensure the data is exactly reproducable though, but any reruns should be very similar.\n",
    "\n",
    "Each function first trains the classifier on our data with no cross validation and default parameters and then scores that model on the test data. This approach gives us a baseline we want to improve. Each function then defines a grid of possible values for each hyperparamater and since doing a cross validated grid search on each parameter combination would take many hours to complete, I opted for a random search through our grid with 5 fold cross validation. Not that this step does not use our test data in any way.\n",
    "\n",
    "Each function then takes the best paramaters from this process and trains the classifier with these new parameters on our data and scores it on our test data. We can then compare this to the default and see if there is any improvement to the hyper parameter modification.\n",
    "\n",
    "Each function returns a tuple with the (default parameter test score, the best hyper parameters it found, the test score of our classifier trained on those best parameters, and the cross validation results).\n",
    "\n",
    "You can also increase the number of iterations of the random search to get more optimal values.\n",
    "\n",
    "My approach to actually getting the final values that I landed on what a combination of running a bunch of randomized searches on different parameter matrices to find what parameters improved accuracy and then a little bit of specific testing on different parameters to fine tune the values.\n",
    "\n",
    "### 3. The list of hyperparameters and brief description of each hyperparameter you tuned in training, their default values, and the final hyperparameter settings you use to get the best result.\n",
    "\n",
    "**Boosted Decision Trees (XGBoost)**\n",
    "- n_estimators: Number of rounds of boosting (default: 100 Final: 100)\n",
    "- max_depth: Maximum tree depth for base learners (default: None Final: 140)\n",
    "- lambda: L2 regularization term on weights (default: 1.0 Final: 2.0)\n",
    "- learning_rate: How dramatic a shift is from one step to another by multiplying by some closed form solution(Default: 1.0 Final: 1.0)\n",
    "- missing: Value in the data which needs to be present as a missing value (Default: 0 Final: 5)\n",
    "- objective: Used to specify our objective function (default: binary:logistic Final: binary:logistic)\n",
    "\n",
    "**Random Forests**\n",
    "- n_estimators: Number of trees in the forest (Default: 100 Final: 650)\n",
    "- boostrap: Whether bootstrap samples (Creating new data by sampling with replacement from exisiting data) are used when building trees (Default: True Final: True)\n",
    "- max_depth: The maximum depth of the tree (Default: None Final: 90)\n",
    "- min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value (Default: 0 Final: 0)\n",
    "- min_samples_leaf: The minimum number of samples required to be at a leaf node (Default: 1 Final: 4)\n",
    "\n",
    "**Support Vector Machines with Gaussian Kernel**\n",
    "- kernel: Specifies the kernel type to be used in the algorithm (Default: 'rbf' Final: 'linear')\n",
    "- gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’ (Default: 'scale' Final: 'scale')\n",
    "- c: Regularization parameter (Default: 1.0, Final: '1.0')\n",
    "\n",
    "### 4. Training error rates, hold-out or cross-validation error rates, and test error rates for your final classifiers. You are also encouraged to report other settings you tried with the accuracy it achieved (please make a table with a column with each hyperparamter and accuracy of configuration of parameters).\n",
    "\n",
    "**Boosted Decision Tree (specifically XGBoost)**\n",
    "- Final Parameters: (n_estimators=100 , max_depth=140, lambda=2.0 , learning_rate=1.0 , missing=5)\n",
    "- Cross validation error rate = 0.15091523\n",
    "- Test error rate = 0.15688164117\n",
    "\n",
    "**Random Forest**\n",
    "- Final Parameters: (n_estimators=650, bootstrap=True, max_depth=90, min_impurity_decrease=0, min_samples_leaf=4)\n",
    "- Cross validation error rate = 0.15863022\n",
    "- Test error rate = 0.15091210613\n",
    "\n",
    "**Support Vector Machines with Gaussian Kernel**\n",
    "- Final Parameters (kernel=linear, gamma=scale, c=1.0)\n",
    "- Cross validation error rate = 0.1472189812\n",
    "- Test error rate = 0.1494379952\n",
    "\n",
    "\n",
    "\n",
    "### 5. Please do your best to obtain the best achievable accuracy for each classifier on given dataset. Note: The amount of effort you put on tuning the parameters will be determined based on the discrepancy between the accuracy you get and the best achievable accuracy on a9a data for each algorithm.\n",
    "\n",
    "I saw marginal improvement with the hyperparameters I found. Usually about 0.25 - 2.5 %, depending on the specific execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f3585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
